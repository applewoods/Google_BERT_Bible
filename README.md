# 구글 BERT의 정석
## 1. Transformer
* RNN과 LSTM의 장기 의존성 문제를 극복하기 위해서 "Attent is All You Need" 논문에서 트랜스포머 아키텍처 제안
* 개념정리 ([링크](https://blog.naver.com/applewoods/222619765689))

## 2. BERT